{"./":{"url":"./","title":"简介","keywords":"","body":"简介 本资源是PyTorch模型训练实用教程的第二版，将为广大朋友带来更丰富的PyTorch学习资源。看到PyTorch模型训练实用教程过了三年，仍旧在发光发热，依旧能帮助到初学者，这令我十分欣慰，但又非常愧疚。为什么？因为深感PyTorch模型训练实用教程的内容不够丰富，不能给初学朋友更多的帮助。为此，《PyTorch实用教程》将弥补第一版的空白，新增丰富的内容，想大家之所想，将提供在线阅读的电子书籍，以及入门友好的、可工业落地的配套代码。 本书整体分三部分，上篇：入门，中篇：应用，下篇：落地。 上篇 PyTorch基础。针对刚入门、非科班、本科生，提供PyTorch介绍，讲解开发环境的搭建，介绍PyTorch核心模块，详解核心功能函数，最后利用所讲解的PyTorch知识点构建一套自己的代码结构，为后续的应用打下基础。 中篇 产业应用。经过上篇，磨了一把好刀，接下来就用它在各领域上大显身手。计划讲解的领域包括，计算机视觉（Computer Vision）的图像分类、图像分割、目标检测、生成对抗网络等；自然语言处理（Natural Language Processing）的部分应用；语音识别；多模态等领域。（由于自己对CV比较熟悉，其他领域欢迎有志之士一同补充） 下篇 工业落地。有了工具，有了场景，接下来就要让它产生价值，变成可用的、好用的产品。本篇会涉及模型加速三板斧：模型量化、模型剪枝、模型蒸馏。此外，还会涉及主流的模型部署框架平台：TensorRT、Openvinno、ONNX等（同上，此处欢迎大家一同补充各平台的部署内容） 相信经过上、中、下篇的学习，可以帮助入门的同学少走很多弯路，快速掌握PyTorch，具备独当一面的能力，能依据实际场景选择算法模型，可以将模型部署应用，形成闭环，全流程打通。 如何使用本资源，请看前言 有任何想法和建议，欢迎与我联系：yts3221@126.com Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"preface.html":{"url":"preface.html","title":"前言","keywords":"","body":"前言 本书背后的故事 《PyTorch实用教程》（第二版），动笔时间为2021年12月16日，什么时候能完结，并不知道。为什么会不是写完再发布？因为担心再不迈出第一步，这本书就会难产。其实，想写这本书已经很久了，至少2年多了吧（见下图），总由于各种原因，迟迟没有开始动笔，索性，采用github在线更新的方式，和各位朋友提前见面吧，你们的反馈是我最大的动力。 为什么是书？ 书籍是人类智慧的结晶，不同于普通的博客、公众号甚至课件，书籍的内容会更加完整和充实，这是受到前些天看到的一个视频有感而发，视频说的是北大退休的韩茂莉教授将在B站上延续她的课堂生涯，将她毕生所学的历史地理知识传授给更多的朋友，在第一个视频里她提到：“但凡具有一种人类形成的，知识性的精华，其实都在书中 。” 看到这句话感触颇深，因为写书的事情一直在脑海中反复回荡，但一次次的被生活的琐碎给拍灭。除了韩老师触动之外，自己这几年的经历也十分契合书写一本书，同时也欢迎有志之士与我一同完善这本《PyTorch实用教程》（第二版）。 一小步 为了不让这本书难产，于是先走出这一小步，让自己踏上书写的征程，至于后面的东西交给时间吧。 为什么写这本书 ​ 这本书是对PyTorch模型训练实用教程的改进优化，是对第一版的内容进行丰富，会增加更多的PyTorch基础，增加丰富的应用案例，同时包含模型部署上线这一关键的知识点。萌生第二版的想法大约在2019年11月，当时距离第一版发布大约一年，期间又对PyTorch有了一个深入的学习了解过程，于是想把这本书继续优化下去，帮助更多的朋友快速掌握PyTorch。可奈于当时精力有限，就迟迟没有动笔，2020年与2021年是忙碌的两年，生活的琐碎以及工作占据了大多数时间，现在（2021年12月16日22:13:10）终于有合适的条件动笔了，一刻不敢拖延，害怕这刚刚燃起的火苗又被琐碎的生活浇灭。 ​ 除了是对第一版的改进，更大的原因是一种责任。由于这几年的经历，让我感到需要出一本高质量的PyTorch书，它一定是大家爱不释手的资料，可以为大家的深度学习道路提供一点便捷，能给大家带来些许改变，这就足够了。第一版发布过去已经三年有余，仍旧能看到她在发光发热，这令我十分欣慰，但又非常愧疚。 第一版仍旧在发光发热 ​ 因为第一版的内容略显简单，内容单薄，缺乏应用案例以及工业部署落地。那么第二版就是为了弥补上述缺点，把PyTorch基础内容完善，增加CV和NLP以及多模态的工业级应用案例，最后讲解模型部署，实现工业落地，这样的教程才是完整的，不是么。 ​ 通过本书，也希望给大家带来一个清晰的深度学习模型应用流程。 读者对象 首先，适合新手，本书上篇将介绍PyTorch最基础的概念，为新手构建知识框架，上篇所有知识点配备Notebook进行图、文、代码，三位一体的学习。 其次，适合初学者，本书中篇将采用PyTorch代码进行各领域应用案例讲解，暂定包括计算机视觉、自然语言处理、语音识别和多模态，同时提供完整的代码框架，开箱即用，无需额外编程，可涵盖大多数朋友的项目需求。 再者，适合中高级朋友，本书下篇将涉及模型加速与部署内容，是算法工程师进阶的必备之路，模型加速包括剪枝、量化、蒸馏，部署暂定包括TensorRT、OpenVINO和ONNX。 上中下篇分别对应不同阶段的学习，相信大多数朋友都能从本书中获益。 如何阅读这本书 本书采用电子书在线更新方式，详见PyTorch实用教程（第二版）-电子书 全书配套代码，详见PyTorch-Tutorial-2nd 由于自身水平、精力有限，书中不可避免的会出现错误，恳请大家指正。同时欢迎大家投稿，例如NLP方向，多模态方向，模型部署，模型加速方向均可。 欢迎联系：yts3221@126.com 致谢 首先感谢老婆的理解与支持（狗头）。 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-1/":{"url":"chapter-1/","title":"第一章 PyTorch 简介与安装","keywords":"","body":"第一章 PyTorch 简介与安装 第一章 PyTorch 简介与安装 1.1 PyTorch 初认识 1.2 环境配置之Anaconda 1.3 环境配置之Pycharm 1.4 环境配置之CUDA&cuDNN 1.5 环境配置之PyTorch系列包 1.6 环境配置之Jupyter Notebook Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-1/1.1-PyTorch-Introduction.html":{"url":"chapter-1/1.1-PyTorch-Introduction.html","title":"1.1 PyTorch 初认识","keywords":"","body":"1.1 PyTorch 初认识 一句话认识PyTorch “An open source machine learning framework that accelerates the path from research prototyping to production deployment.” ——选自 https://pytorch.org/ PyTorch是一个帮助大家加速算法模型研究到产品部署的开源机器学习框架。 PyTorch历史 FAIR（ Facebook AI Research，Facebook人工智能研究院 ）于2017年初发布PyTorch，PyTorch的命名可以拆为Py（Python）与Torch。Py就是python语言，Torch是一款早期的深度学习框架，所以PyTorch是在Torch基础上用python语言重新打造的一款深度学习框架。 那么什么是Torch呢？Torch是纽约大学在2002年开发的深度学习框架。Torch虽然很好用，但是它采用了一门较为小众的语言——Lua语言作为接口。这明显地增高了Torch的使用门槛，想要使用Torch必须学习一门新的编程语言，这让大家都望而却步。 好在Torch的幕后团队也意识到这一点，于是团队采用python语言作为接口对Torch进行了重构，于是就有了PyTorch。 PyTorch代码最早公开可追溯到2016年8月24日的v0.1.1（https://github.com/pytorch/pytorch/tags?after=v0.1.4） 随后 •2017年1月正式发布PyTorch •2018年4月更新0.4.0版，支持Windows系统 •2018年11月更新1.0稳定版，已GitHub 增长第二快的开源项目 ...... 对PyTorch版本的更新感兴趣的朋友，可以关注 PyTorch-Tags，这里是最权威版本更新信息来源，要比官方文档还要快。 PyTorch必备网站 要熟悉PyTorch，不得不熟悉PyTorch的一些官方网站，以及这些网站的使用。下面列举几个实用的PyTorch网站。 官网 https://pytorch.org/ 官网包含权威的PyTorch介绍、PyTorch官方文档、生态资源等信息。例如Get Started中，可以获取权威的安装信息。例如，特定版本下，windows系统，所支持的CUDA版本是多少，这点非常关键，往往GPU用不起来，就是CUDA版本不匹配。 除了最新稳定版，还可以下载历史版本的whl文件，进行离线安装（网速不好的朋友，建议手动下载whl，然后进行安装）。历史版本whl的在哪那里下载呢？ 是需要挖掘的，网址在上图的windows系统、Pip安装、Python、CUDA条件下才会出现，它是：https://download.pytorch.org/whl，点击torch，就可以发现所有历史版本都在这里可以找到，并且命名都有良好的规范，这里不过多介绍，在安装部分再详细讲解。 除了Get Started栏目，其它栏目也是很好的学习资料。 Ecosystem：PyTorch生态系统资源库，里面收录生态系统内的资源，也欢迎大家加入并贡献资源，里边有CV数据增强开源库——albumentations、FB的目标检测和分割算法库——detectron2、优秀的部署平台——onnxruntime等等 Mobile：移动端PyTorch实现方案及资源。 Blog：PyTorch相关新闻。 Tutorials：案例教程，这里面都是个人提供的、针对某一个应用的demo级的教程。包含如下方向，对于新手来说，可以看一下，了解个大概，但里面的代码多是demo，无法开箱即用于项目应用，这也是本书第二部分将会弥补的地方，敬请期待。 Docs：PyTorch API官方文档, 这也是我一直首推的学习资料，PyTorch的文档非常友好，可以查阅不同版本，各个API都有详细介绍，大多都有代码案例，PyTorch的基础部分主要从这里开始进行讲解。Docs下还会分PyTorch、torchvision、torchaudio、torchtext等，大家需要针对性的检索。 Resource：这里包含各种资源，如社区、新闻报道、论坛、ModelHub资源等等。 PyTorch发展趋势 为什么要学PyTorch？ 因为PyTorch发展迅猛，已经在多方面荣登深度学习框架第一的宝座，学术界绝大多数论文都有PyTorch实现，想要紧跟技术，利用新模型进行科学研究，进行项目开发的，不得不跟随学术界的趋势，所以可以看到PyTorch席卷各界。 PyTorch称王，TensorFlow答应么？一起来看看几个数据。 图1： 各大顶会期刊中，使用PyTorch的论文数量占PyTorch+TensorFlow的百分比。其实就是 p / (p+t)，这里有一个分界点就是50%，当达到50%时，说明PyTorch与TensorFlow平分秋色，当大于50%时说明PyTorch已经超过TF，而当数据超过75%，表明PyTorch已经是TF的两倍。从这个趋势可以发现转折点在2018-2019年之间发生，现在已经2021年末了，哪个框架是学术界的带头大哥？ 图2：这幅图对比的是PyTorch与TF的决定数量，可以看到TF的份额被PyTorch一步步蚕食，实线代表的PyTorch持续上扬，TF的虚线不断下探。 图片出自：https://horace.io/pytorch-vs-tensorflow/ 通过学术界的论文情况就可以说明PyTorch是未来的大势所趋，虽然说早期PyTorch在工业部署上并不如TensorFlow，但是如今PyTorch推出了libtorch，TorchServe，以及各类优秀的，适配性良好的部署框架层出不穷，如TensorRT、OpenVINO、ONNX等，都可以帮助PyTorch进行快速部署 感觉PyTorch是得学术界得天下，先让科研工作者用爽了，新模型都是PyTorch实现的，工业界的朋友总不能不用最新的算法、模型吧，只能纷纷转向PyTorch了。因此，相信大家选择使用PyTorch进行深度学习、机器学习模型开发，一定能加速你的算法模型开发，也印证了PyTorch的主旨——“An open source machine learning framework that accelerates the path from research prototyping to production deployment.” Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-1/1.2-Anaconda.html":{"url":"chapter-1/1.2-Anaconda.html","title":"1.2 环境配置之Anaconda","keywords":"","body":"1.2 环境配置之Anaconda 工欲善其事必先利其器，想要使用PyTorch进行模型开发，必须要搭建好开发环境。听到环境安装，是否大家已经瑟瑟发抖？是否都被某些不知名的报错卡住好几天，百度各种搜不到答案，各种文章的方案都不适用？ 为了解决大家环境安装的苦恼，本章将从Python虚拟环境、Anaconda、Pycharm、CUDA、CuDNN的背景说起，给大家构建系统的认识，理清各软件之间的关系，为大家呈现一个清晰的、完整的开发环境配置。 1.1节中提到过，PyTorch是基于Python为接口提供给用户使用，因此Python语言是我们的核心，PyTorch对于python只是一个工具包（library），通过import torch的方式使用而已。因此想要搭建完整的PyTorch开发环境，其实是搭建完整的Python开发环境，同时安装上PyTorch这个工具库。 虚拟环境 为了使用PyTorch，先搞定Python环境安装。提到Python环境，就不得不讲python虚拟环境（virtual environment）的概念了。python虚拟环境是为了解决众多的工具包之间版本冲突而设计的一个纯净开发环境，简单地，可创建多个虚拟环境，不同的环境中使用不同的工具包，例如虚拟环境1中安装pytorch 1.6， 虚拟环境2中安装的是pytorch0.4，当需要用老版本pytorch时，切换到虚拟环境2，然后调用虚拟环境2中的解释器——python.exe 运行你的.py代码。当需要用pytorch1.6时，就切换到虚拟环境1，调用虚拟环境1中的python.exe运行代码。这样就很好的解决工具包版本冲突问题。 解释器 这里提到一个概念，解释器——python.exe。 解释器就是人类与CPU之间的桥梁，人写的高级语言，如print(\"Hello World\")，CPU是读不懂的，CPU只能读0/1形式的二进制文件，这时就需要一个翻译官——python.exe， python.exe读取.py文件，解释成二进制文件，让CPU读懂，并运行。这就是解释器的作用，更直观的例子，python3的解释器是无法翻译python2语法的语句print \"Hello World\"的，因此不同的python.exe其实就对应了不同的环境。 Anaconda Anaconda是为方便使用python而建立的一个软件包，其包含常用的250多个工具包，多版本python解释器和强大的虚拟环境管理工具，所以Anaconda得名python全家桶。Anaconda可以使安装、运行和升级环境变得更简单，因此使用它作为Python虚拟环境管理工具。 安装非常简单，首先进入 anaconda官网，点击“Get Started\", 点击”download anaconda installers“，看到如下图所示的信息，选择你对应的操作系统下载，安装即可。 安装完毕，可以尝试创建一个你的虚拟环境，这里需要注意创建环境的时候就要决定pyhon的版本，而pytorch的安装对python的版本有要求，所以大家先到pytorch官网看看要装的pytorch版本所支持的python版本，然后再回来创建虚拟环境。这里演示pytorch最新版本1.10的安装。由于1.10是支持python3.6/3.7/3.9的（通过 1.1介绍过的神奇网站找到信息），在这里用python3.6作为python版本进行创建虚拟环境。 >>> conda create -n pytorch-1.10-gpu python=3.6 这里的pytorch-1.10-gpu就是虚拟环境的名称，激活（activate）时的标识，同时也会在anaconda安装目录下创建pytorch-1.10-gpu的文件夹，在该文件夹存放本环境所有工具包、解释器，如下图所示： 虚拟环境创建好之后，可以看看这个文件夹里都有些什么，先来看解释器： D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\python.exe ，大家可以双击它，就可以看到解释器的相关信息，后续用到pytorch_1.10_gpu这个环境的时候，也是用这个.exe进行运行.py文件，后面用pycharm运行代码的时候会给大家callback。 到此，一个纯净的虚拟环境就创建好了，接下来需要激活（activate）这个环境，然后再里面进行各种工具包的安装。这里先暂停一下，先去看另外一个工具——Pycharm的安装及使用。 anaconda常用命令 创建环境：conda create -n your_env_name python=X.X （X.X为python版本） eg: conda create -n pytorch_tutorial python=3.7 激活环境：source activate your_env_name eg: source activate pytorch_tutorial 退出环境：source deactivate 删除环境：conda remove -n your_env_name –all eg: conda remove -n pytorch_tutorial --all 查看已有虚拟环境：conda env list / conda info -e （推荐大家自行了解更多anaconda命令） 有了anaconda帮助我们管理虚拟环境以及python工具包，接下来就可以安装IDE，用来管理项目了。请看Pycharm安装 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-1/1.3-Pycharm.html":{"url":"chapter-1/1.3-Pycharm.html","title":"1.3 环境配置之Pycharm","keywords":"","body":"1.3 环境配置之Pycharm Pycharm——强大的python IDE Pycharm——强大的python IDE，拥有调试、语法高亮、Project管理、代码跳转、智能提示、版本控制等功能。有社区版和专业版区分，社区版为免费的，专业版需要付费，不过在校学生可通过edu邮箱进行注册，获取免费的专版使用。当然大家可以有其他方法完成购买。专业版相对于社区版功能更丰富一些，这里我采用的是pycharm.2019专业版，基础功能没有大的区别，用于演示还是OK的。 Pycharm的安装： 官网下载安装包 https://www.jetbrains.com/pycharm/ 运行pycharm-professional-2019.2.exe 选择路径，勾选Add launchers dir to the PATH，等待安装完成 激活部分：略。 这里主要讲如何创建项目，以及关联前面创建的虚拟环境pytorch_1.10_gpu。 打开pycharm，左上角的File可选择New，或者Open，如果已经有一个文件夹下有相关.py代码，那么就用Open对应的文件夹即可。这里假设已经存在pytorch-tutorial-2nd文件夹，找到它，Open即可。 我们找到pytorch-tutorial-2nd\\code\\chapter-1\\01-hello-pytorch.py，发现import torch下面有个红色波浪线，鼠标放上去会提示“No Module named torch\"，表明当前环境里并没有torch这个工具包。可好像我们并没有为当前.py文件设定好用哪个一解释器不是？所以我们先将当前项目pytorch-tutorial-2nd的虚拟环境设置为刚刚创建好的pytorch_1.10_gpu，然后再在pytorch_1.10_gpu里安装上pytorch即可。 左上角File--> Settings-->Project:pytorch-tutorial-2nd-->Project Interpreter， 然后如下图找到对应的python.exe，之后选中，点击OK,再次点击OK。就完成了虚拟环境与项目的关联，接着就可以安装pytorch了。 到这里，大家可以尝试运行 pytorch-tutorial-2nd\\code\\chapter-1\\01-hello-pytorch.py，会提示 D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\python.exe E:/pytorch-tutorial-2nd/code/chapter-1/01-hello-pytorch.py Traceback (most recent call last): File \"E:/pytorch-tutorial-2nd/code/chapter-1/01-hello-pytorch.py\", line 9, in import torch ModuleNotFoundError: No module named 'torch' Process finished with exit code 1 这里是完整的Pycharm控制台信息，我们可以看到第一行先是解释器（即对应了我们创建的虚拟环境），然后是执行的.py文件，接着是报错信息，提示没有torch这个module，下一小节我们来就来安装这个module。 pycharm拓展 pycharm是很好用的IDE，这里面提供很多快捷键，希望大家可以熟悉使用这些快捷键，例如常用的 批量注释：Ctrl + / 快速查看文档：Ctrl + q 搜索：Ctrl+f 运行：Shift + F10 Tab / Shift + Tab 缩进、不缩进当前行 Ctrl + D 复制选定的区域或行 Ctrl + Y 删除选定的行 更多功能推荐大家自行了解一下pycharm的基础使用，相信它一定是你的高效生产力。pycharm也有一些出名的教程，例如《pycharm 中文指南》pycharm中文指南。 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-1/1.4-CUDA&cuDNN.html":{"url":"chapter-1/1.4-CUDA&cuDNN.html","title":"1.4 环境配置之CUDA&cuDNN","keywords":"","body":"1.4 环境配置之CUDA&cuDNN 有了python环境，有了开发环境，下面马上到主角登场。PyTorch登场前，针对GPU版，还需要额外安装一些东西。 从1.1我们知道PyTorch的安装可根据设备类型分为GPU版或CPU版。 CPU 对于CPU版本直接通过pip或者anaconda命令安装即可，如： >>> pip3 install torch torchvision torchaudio 具体的命令可查阅：https://pytorch.org/get-started/locally/ 官网上给出的命令其实安装了3个包，分别是torch, torchvision,torchaudio，这命令会根据当前系统自动选择对应python版本的whl进行安装，不需要用户操心。但，如果网速不好，或者需要离线安装，这时可以考虑下载whl包然后自行安装，下载whl的链接：https://download.pytorch.org/whl/torch/ pytorch与torchvision版本匹配 若是手动下载的whl，需要注意pytorch与torchvision之间版本对应关系，这个可以到torchvision Github查看，这点非常重要，CV中一些报错就是因为torchvision与pytorch版本不匹配导致的。这里就copy过来，大家参考好了。 torch torchvision python main / nightly main / nightly >=3.6, 1.10.0 0.11.1 >=3.6, 1.9.1 0.10.1 >=3.6, 1.9.0 0.10.0 >=3.6, 1.8.2 0.9.2 >=3.6, 1.8.1 0.9.1 >=3.6, 1.8.0 0.9.0 >=3.6, 1.7.1 0.8.2 >=3.6, 1.7.0 0.8.1 >=3.6, 1.7.0 0.8.0 >=3.6, 1.6.0 0.7.0 >=3.6, 1.5.1 0.6.1 >=3.5, 1.5.0 0.6.0 >=3.5, 1.4.0 0.5.0 ==2.7, >=3.5, 1.3.1 0.4.2 ==2.7, >=3.5, 1.3.0 0.4.1 ==2.7, >=3.5, 1.2.0 0.4.0 ==2.7, >=3.5, 1.1.0 0.3.0 ==2.7, >=3.5, 0.2.2 ==2.7, >=3.5, 举一反三，torchaudio、torchtext同理。 GPU版本 深度学习能火，正式因为有了强大的GPU支撑，自然地，绝大多数情况下我们会安装GPU版本的pytorch。目前PyTorch不仅支持NVIDIA的GPU，还支持AMD的ROMc的GPU。不过我们还是以N卡为例，毕竟N卡还是主流，A卡仍需努力。 对于N卡，什么型号是pytorch支持的呢？首先，需要计算能力（compute capability）≥3.0的GPU。很多地方都会看到计算能力≥3.0，理论出自哪呢？ 我在官方文档里找到了出处文档 It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0 那么问题来了，怎么知道自己的GPU的copute capability呢？请看NVIDA文档,选择你对应的系列，找到对应型号。 举几个例子： GPU 计算能力 GeForce RTX 2080 7.5 GeForce RTX 2070 7.5 GeForce RTX 2060 7.5 GeForce GTX 1080 6.1 GeForce GTX 1070 6.1 GeForce GTX 1060 6.1 其实，只要是近几年购买的N卡都是没有问题的。确定了显卡是支持的，接下来就要决定一个非常重要事情，就是选中对应的CUDA版本进行安装。 CUDA ​ CUDA(ComputeUnified Device Architecture)，是NVIDIA推出的运算平台。 CUDA是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。 与之配套的是cuDNN, NVIDIA cuDNN是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。NVIDIA cuDNN可以集成到更高级别的机器学习框架中。 细心的朋友在PyTorch官网就能发现， Compute Platform中并不给出显卡型号，而是给出CUDA版本，这就要求我们安装特定版本的CUDA，才能使用特定版本的PyTorch。例如PyTorch 1.10 只支持CUDA 10.2, CUDA 11.3，以及CUDA 11.1。为什么这里用了以及呢？ 因为在官网上并没有显示CUDA 11.1，但是在https://download.pytorch.org/whl/torch，搜索，可以看到11.1的whl。 在这里选择的是10.2版本进行安装，CUDA下载通过官网，官网通常只显示最新版本cuda，这里需要大家进入具体的版本下载界面，拖到底部，找到： Archive of Previous CUDA Releases 接着可以找到对应的CUDA版本，进入下载即可，这Installer Type 有 exe (network) 和 exe (local)两种选择，我们选local的方式，下载2.6G的cuda_10.2.89_441.22_win10.exe即可。 安装方式十分简单，一直下一步即可，只需要记住安装到了哪里，这里默认路径为 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2 下面来测试一下CUDA安装是否成功，可以打开命令窗，进入C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin，然后输入 nvcc -V cuDNN 有了CUDA平台，还需要安装cuDNN，cuDNN全称为NVIDIA CUDA Deep Neural Network (cuDNN) 。它是一个深度神经网络的加速库，里边实现了神经网络常用的操作，并且是高度优化的，可以极大地榨干NVIDA显卡的性能，因此用N卡都会用cuDNN库。 cuDNN库的安装非常简单，与其说是安装，不如说是下载库文件，放到CUDA所在的目录下。具体步骤如下： 打开网址：https://developer.nvidia.com/cudnn，点击右上角，需要注册，再登录。 登录后，点击Download cuDNN，跳转到下载页面，选择好cudnn版本，操作系统版本，即可开始下载 将下载好的压缩包cudnn-10.2-windows10-x64-v8.2.4.15.zip 解压 分别将bin、include、lib\\x64下的文件分别对应拷贝到C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2文件夹下的bin、include、lib\\x64下 打开命令窗口，在C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\extras\\demo_suite文件夹中分别执行bandwidthTest.exe和deviceQuery.exe。观察到Result=PASS即可 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-1/1.5-PyTorch-install.html":{"url":"chapter-1/1.5-PyTorch-install.html","title":"1.5 环境配置之PyTorch系列包","keywords":"","body":"1.5 环境配置之PyTorch系列包 虚拟环境，Pycharm，CUDA，cuDNN均已准备好，现在终于可以安装PyTorch了，加油，就快成功啦。 现在，通过命令窗口，进入（激活）虚拟环境 E:\\pytorch-tutorial-2nd>conda activate pytorch_1.10_gpu (pytorch_1.10_gpu) E:\\pytorch-tutorial-2nd> 可通过以下命令安装 pip3 install torch==1.10.1+cu102 torchvision==0.11.2+cu102 torchaudio===0.10.1+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html 可以看到通过pip安装，也是下载我们提到的神奇网站里的whl文件，这时大家可以根据自己的网速决定是采用pip还是自行下载的方法。 如果网速不好的话，推荐通过神奇的网站——https://download.pytorch.org/whl/torch 搜索对应的whl进行下载。然后pip install *.whl就行。 对于pip，建议大家添加镜像源。例如，清华镜像源或者中科大镜像源，这样安装python工具包的下载速度会快很多，请自行百度如何添加清华镜像源。 安装完毕，再回到pycharm，运行 pytorch-tutorial-2nd\\code\\chapter-1\\01-hello-pytorch.py，可以看到 D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\python.exe E:/pytorch-tutorial-2nd/code/chapter-1/01-hello-pytorch.py Hello World, Hello PyTorch 1.10.1+cu102 CUDA is available:True, version is 10.2 device_name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design Process finished with exit code 0 表示pytorch环境安装完毕，此时我们也可以再次打开pycharm的解释器配置，可以看到当前的解释器（虚拟环境）下，拥有的相关工具包，这个界面也是后续大家检查当前环境工具包版本常用的工具，请收藏。 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-1/1.6-JupyterNotebook-install.html":{"url":"chapter-1/1.6-JupyterNotebook-install.html","title":"1.6 环境配置之Jupyter Notebook","keywords":"","body":"1.6 环境配置之Jupyter Notebook 什么是 jupyter notebook 经过前几个小节，大家已经具备了PyTorch开发环境，但本教程需要照顾初学者使用代码，让刚入门的同学有更好的代码学习体验。因此，在上篇，主要采用Jupyter Notebook进行代码演示。 注意：\"Notebook只建议用于学习目的，不建议用于项目开发\" * 3， 重要事说三遍！ 为什么？这是由于notebook自身定位决定的，先来看看jupyter notebook 的定义“The Jupyter Notebook is a web application for creating and sharing documents that contain code, visualizations, and text. It can be used for data science, statistical modeling, machine learning, and much more.”——官网 Jupyter notebook 是一个网页应用，在这个网页上可以编写代码、做可视化、写文本，这就是非常好的教学展示平台。可以在上面进行概念描述、配上代码、运行结果，并且可以按小段进行代码运行，给用户更多的交互体验，便于用户理解代码细节。 基于此，上篇主要采用notebook进行代码讲解，到了中篇，基于完整的项目代码框架进行应用开发。 关于jupyter与jupyter notebook的关系，请大家查看官网（以下开始，notebook 指代 jupyter notebook） notebook 运行逻辑 notebook不是一个简单的web应用程序，它还需要关联指定的kernel，用于执行我们的代码。相信刚接触notebook的朋友大多都被notebook, kernel的概念搞的一头雾水。如果上述概念理不清楚，就更不清楚如何配置kernel，选择指定的虚拟环境了。 下面，我们先来看看notebook的结构 图片来自官网 图中左边是用户写的代码，传输到中间的Jupyter server, server本身不能执行代码（python.exe干的活，server是不会的），server把代码传给Kernel，Kernel才是真正干活，执行代码的地方。Kernel执行完代码，把结果返回给server，再返回到用户的网页。 从图中可以看出Kernel不仅可以是python.exe，也可以是其他语言的解释器，如Julia, R等，更多kernel可以看支持的kernel列表. 通过上述示意图我们就知道了，在pytorch开发中，kernel其实就是某一个python解释器——python.exe，我们需要让当前的notebook启用对应的kernel，来进入相应的虚拟环境，这样才能运行代码。 notebook 安装 理清概念，下面进行notebook安装，我们续期是正确调用pytorch_1.10_gpu这个虚拟环境来执行notebook上的代码。 安装过程分3步： 进入虚拟环境：conda activate pytorch_1.10_gpu 安装ipykernel工具包（安装jupyter）: pip install jupyter 添加kernel到notebook： python -m ipykernel install --user --name pytorch_1.10_gpu (意思是，将python这个kernel添加到jupyter的kernel中，由于当前已经在虚拟环境中，所以第一个python表示的含义是：D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\python.exe；而pytorch_1.10_gpu是kernel的别名，用于区分不同的kernel，这里建议与虚拟环境名称保持一致就好) 启动 在命令窗中执行jupyter notebook就可以打开web应用了，网址为:http://localhost:8888; 这里默认端口为8888，如果你再次启动一个jupyter notebook，可以看到端口号变为了8889，即它是另外一个web服务。 进入之后，我们可以看到有一个/目录，我们需要找到我们的notebook文件进行打开，这里有一个小技巧，就是进入到指定文件夹后，再运行notebook，这样notebook的路径就进入了想要的文件夹。 配置kernel 我们进入 chapter-1/02-notebook-demo.ipynb，点击run，可发现如下报错 --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) in 7 \"\"\" 8 ----> 9 import torch 10 11 print(\"Hello World, Hello PyTorch {}\".format(torch.__version__)) ModuleNotFoundError: No module named 'torch' 告诉我们找不到torch这个包，这很明显，使用的kernel不是我们的D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\python.exe。 下面我们来设置一下，方法很简单： 再次运行，可看到以下信息，表明notebook的环境就配置好了。 Hello World, Hello PyTorch 1.10.1+cu102 CUDA is available:True, version is 10.2 device_name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design 实用插件——jupyter_contrib_nbextensions 原生的notebook还是缺点意思，这里推荐大家安装jupyter_contrib_nbextensions插件，jupyter_contrib_nbextensions提供了非常丰富的功能，例如代码折叠、分段折叠、代码自动补全、字体大小、行号显示、目录索引等等，详见下图 插件安装十分简单，打开命令窗，进入虚拟环境，分别依次执行 : pip install jupyter_contrib_nbextensions jupyter contrib nbextension install --user 然后重启notebook，就可以看到导航栏里有Nbextensions，大家可以根据自己的喜好进行调整，更多内容请查看Github Notebook 快速上手 notebook所使用的文件格式为.ipynb，jupyter会将.ipynb转为json进行保存，这样便于版本记录以及分享。 例如下图是用sublime打开的 02-notebook-demo.ipynb 下面，我们来研究notebook界面和常用的操作。 界面中需要认识的几个模块分别是：菜单栏、工具栏、单元格（cell） 菜单栏：用得最多的是Kernel，用于中断程序、重启解释器环境、切换解释器等；其它按键顾名思义。 工具栏：一些功能的按钮，高手都是用快捷键的。 单元格：这就是承载信息的地方，cell可分为code cells, markdown cells, raw cells。用得最多的是code cells和markdown cells。 右上角有一个小圆圈，用于观察当前kernel运行状态，如果是实心的，表明kernel正在运行某个cell，被运行的cell以及等待运行的cell的左边会有一个* notebook 的两种模式 Notebook中的单元，有两种模式：命令模式(Command Mode)与编辑模式(Edit Mode)，在不同模式下我们可以进行不同的操作。 命令模式：cell的边框为蓝色，此时可对cell进行操作。在编辑模式下，按esc键进入命令模式。 编辑模式：cell的边框为绿色，此时可在单元格内编辑代码或文档。在命令模式下，按enter或return键进入编辑模式。 常用快捷键 在命令模式下，按下“h”键，就会弹出快捷键的介绍，但是太多了，不方便初学者使用，这里总结一些常用的，实用的快捷键供大家参考。 命令模式： 插入单元格： A 键上方插入，B 键在下方插入 合并单元格：选中多个单元格，Shift + M 显示行号：L 删除单元格：连续按两次D 剪切单元格：X。 通常我用X代替删除，毕竟只用按一个键，哈哈。 复制粘贴单元格： C/V 撤销删除的单元格：要撤消已删除的单元格，请按 Z 键 编辑模式： 运行单元格：Ctrl + Enter 运行并创建新单元格：Alt + Enter 分割单元格：光标放到想要分割的地方，Ctrl + Shift + - 函数详情：Shift+Tab （注意，要把模块导入才会提示函数详情！） 请大家将以上快捷键都试用一遍，这些是高频快捷键，下面给大家列举所有快捷键，请收藏。 下面再介绍两个神奇操作，分别是在单元格中执行shell命令以及magic操作。 请自行尝试!+shell命令进行体会。 magic commands Magic关键字是 IPython 的高级用法，如%matplotlib将matplolib设置为交互式 %和%%分别代表 行Magic命令 和 单元格Magic命令 演示一个魔法命令 %%timeit %%timeit a = [] for i in range(10): a.append(i) 858 ns ± 50.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 表示将代码段运行了100万次，并统计运行时间。 更多更全的magic commands请看这里Jupyter 魔术命令（magic commands） 更多奇淫技巧推荐大家看看Jupyter Notebook 有哪些奇技淫巧？ 更多官方信息请查看Jupyter Notebook docs Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-2/":{"url":"chapter-2/","title":"第二章 PyTorch 核心模块","keywords":"","body":"第二章 PyTorch 核心模块 上一章，对PyTorch的历史进行介绍，对开发环境的安装进行了详细的讲解。 本章将对PyTorch代码结构进行梳理，介绍核心模块，为后面应用PyTorch打下基础。 第二章 PyTorch 核心模块 2.1 PyTorch 模块结构 2.2 新冠肺炎分类 2.3 核心数据结构——Tensor 2.4 张量的相关函数 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-2/2.1-module-tree.html":{"url":"chapter-2/2.1-module-tree.html","title":"2.1 PyTorch 模块结构","keywords":"","body":"2.1 PyTorch 模块结构 上一章安装好的PyTorch是一个庞大的python库，里边包含几十个模块，这一小节就来了解都有哪些模块，每个模块代码在哪里，对应文档在哪里。从而帮助大家具象化PyTorch，清楚地知道所用的PyTorch函数、模块都在哪里，是如何调用的。 你的代码在哪？ 很多朋友应该都用过pip/conda install 进行一键安装，但你的工具库代码装到哪里并不清楚。使用的时候也知道import *，但具体引用的功能函数又是如何实现的，是模糊的。 为了让大家知道自己调用的是什么，我们先来看你安装的pytorch在哪里。上一章案例中，我们装的pytorch在：D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\Lib\\site-packages\\torch 如果pycharm中配置好了虚拟环境，大家也可以通过pycharm的快捷键，快速定位到这个文件夹。方法是，找到import torch这一行代码，按住Ctrl键，鼠标左键单击torch，就可以跳转到D:\\Anacondadata\\envs\\pytorch1.10_gpu\\Lib\\site-packages\\torch__init.py 文件。 可以看到torch文件夹中有一系列子文件夹，我们平时常用的函数都藏在这里面了，下面挑些重点注意介绍。 _pycache_ 该文件夹存放python解释器生成的字节码，后缀通常为pyc/pyo。其目的是利用空间换时间，对应的模块直接读取pyc文件，而不需再次将.py语言转换为字节码的过程，从此节省了时间。 从文件夹名称可知，它是一个cache，缓存，如果需要，我们当然可以删掉它。更多关于pycache的内容，建议额外阅读：https://www.python.org/dev/peps/pep-3147/#proposal _C 从文件夹名称就知道它和C语言有关，其实它是辅助C语言代码调用的一个模块，该文件夹里存放了一系列pyi文件，pyi文件是python用来校验数据类型的，如果调用数据类型不规范，会报错。更多pyi知识，请查阅PEP 8 -->.pyi files that are read by the type checker in preference of the corresponding .py files. PyTorch的底层计算代码采用的是C++语言编写，并封装成库，供pytorch的python语言进行调用。这点非常重要，后续我们会发现一些pytorch函数无法跳转到具体实现，这是因为具体的实现通过C++语言，我们无法在Pycharm中跳转查看。 include 上面讲到pytorch许多底层运算用的是C++代码，那么C++代码在哪里呢？ 它们在这里,在torch/csrc文件夹下可以看到各个.h/.hpp文件，而在python库中，只包含头文件，这些头文件就在include文件夹下。 lib torch文件夹最重*3的一个模块，torch文件夹占3.2GB，98%的内容都在lib中，占了3.16GB。啥？装了那么大的pytorch，几乎都在lib里面了，倒要看看里面是什么宝贝。 lib文件夹下包含大量的.lib .dll文件（分别是静态链接库和动态链接库），例如大名鼎鼎的cudnn64_7.dll（占435MB）， torch_cuda.dll（940MB）。这些底层库都会被各类顶层python api调用。这里推荐大家自行了解什么是静态链接库和动态链接库。 autograd 该模块是pytorch的核心模块与概念，它实现了梯度的自动求导，极大地简化了深度学习研究者开发的工作量，开发人员只需编写前向传播代码，反向传播部分由autograd自动实现，再也不用手动去推导数学公式，然后编写代码了（很多朋友可能不知道，在早期的深度学习框架中是没有这个功能的，例如caffe，它需要手动编写反向传播的公式代码） nn 相信这个模块是99%pytorch开发者使用频率最高的模块，搭建网络的网络层就在nn.modules里边。nn.modules也将作为一章独立展开。我们可以到D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\Lib\\site-packages\\torch\\nn\\modules里面看看是否有你熟悉的网络层？ onnx pytorch模型转换到onnx模型表示的核心模块，进入文件夹可以看到大量的opset**.py， 这里留下一个问题，各版本opset是什么意思？有什么区别？ optim 优化模块，深度学习的学习过程，就是不断的优化，而优化使用的方法函数，都暗藏在了optim文件夹中，进入该文件夹，可以看到熟悉的优化方法：adam、sgd、asgd等。以及非常重要的学习率调整模块：lr_scheduler.py。本模块也将采用独立一章进行详细剖析。 utils utils是各种软件工程中常见的文件夹，其中包含的是各类常用工具，其中比较关键的是data文件夹，tensorboard文件夹，这些都将在后续章节详细展开。第三章将展开data里的dataloader与dataset等数据读取相关的模块。 其他文件夹不再一一介绍，大家可以到官方文档查看。 以上是torch库，针对不同的应用方向，pytorch还提供了torchvision\\torchtext\\torchaudio等模块，本书重点对torchvision进行剖析，其它两个模块类似。 torchvision 同理，我们来到D:\\Anaconda_data\\envs\\pytorch_1.10_gpu\\Lib\\site-packages\\torchvision文件夹下看看有什么模块。 datasets 这里是官方为常用的数据集写的数据读取函数，例如常见的cifar, coco, mnist,svhn,voc都是有对应的函数支持，可以愉快的调用轮子，同时也可以学习大牛们是如何写dataset的。 models 这里是宝藏库，里边存放了经典的、可复现的、有训练权重参数可下载的视觉模型，例如分类的alexnet、densenet、efficientnet、mobilenet-v1/2/3、resnet等，分割模型、检测模型、视频任务模型、量化模型。这个库里边的模型实现，也是大家可以借鉴学习的好资料，可以模仿它们的代码结构，函数、类的组织。 ops 视觉任务特殊的功能函数，例如检测中用到的 roi_align, roi_pool，boxes的生成，以及focal_loss实现，都在这里边有实现。 transforms 数据增强库，相信99%的初学者用到的第一个视觉数据增强库就是transforms了，transforms是pytorch自带的图像预处理、增强、转换工具，可以满足日常的需求。但无法满足各类复杂场景，因此后续会介绍更强大的、更通用的、使用人数更多的数据增强库——Albumentations。 通过torchvision\\transforms\\transforms.py , 可以看到 torchvision包含了这些功能。 __all__ = [\"Compose\", \"ToTensor\", \"PILToTensor\", \"ConvertImageDtype\", \"ToPILImage\", \"Normalize\", \"Resize\", \"Scale\", \"CenterCrop\", \"Pad\", \"Lambda\", \"RandomApply\", \"RandomChoice\", \"RandomOrder\", \"RandomCrop\", \"RandomHorizontalFlip\", \"RandomVerticalFlip\", \"RandomResizedCrop\", \"RandomSizedCrop\", \"FiveCrop\", \"TenCrop\", \"LinearTransformation\", \"ColorJitter\", \"RandomRotation\", \"RandomAffine\", \"Grayscale\", \"RandomGrayscale\", \"RandomPerspective\", \"RandomErasing\", \"GaussianBlur\", \"InterpolationMode\", \"RandomInvert\", \"RandomPosterize\", \"RandomSolarize\", \"RandomAdjustSharpness\", \"RandomAutocontrast\", \"RandomEqualize\"] 通过上面的内容，相信大家对所安装的代码结构有了清晰认识，也知道自己将调用的代码函数都在哪里，已经为下一步工作打好基础，下一节我们极简的代码，完成第一个深度学习任务—— 新冠肺炎X光分类 。其目的在于为大家搭建模型训练框架，构建各模块的认识，为后续核心模块讲解铺平道路。 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-2/2.2-covid-19-cls.html":{"url":"chapter-2/2.2-covid-19-cls.html","title":"2.2 新冠肺炎分类","keywords":"","body":"2.2 新冠肺炎X光分类 上一节，我们学习了pytorch python API的结构，本节将以一个具体的案例介绍pytorch模型训练流程，并提出一系列问题，供大家思考。当然，这些问题也是本书后续章节一一解答的内容。 相信绝大多数朋友接触过或者看到的第一个Hello Word级图像分类都是Mnist，思来想去觉得还是换点东西，于是选择了当下与所有人都息息相关的案例——新型冠状病毒肺炎（Corona Virus Disease 2019，COVID-19），简称“新冠肺炎”。关于新冠的背景，已经无需多言，口罩、绿码、核酸检测已经融入了我们的生活。因此，想让大家更进一步的了解COVID-19，所以选用此案例。当然，最重要的目的是要了解pytorch如何完成模型训练。 案例背景 2020年1月底2月初的时候，新冠在国内/外大流行。而确定一个人是否感染新冠肺炎，是尤为重要的事情。新冠肺炎的确诊需要通过核酸检测完成，但是核酸检测并不是那么容易完成的，需要医护人员采样、送检、PCR仪器上机、出结果、发报告等一系列复杂工序，核酸检测产能完全达不到当时的检测需求。当时，就有医生提出，是否可以采用特殊方法进行诊断，例如通过CT、X光的方法，给病人拍个片，几分钟就能看出结果，比核酸检测快了不少。于是，新冠肺炎患者的胸片X光数据就不断的被收集，并发布到网上供全球科学家使用，共同抗击新冠疫情。这里就采用了https://github.com/ieee8023/covid-chestxray-dataset上的数据，同时采用了正常人的X光片，来自于：https://github.com/zoogzog/chexnet。 由于本案例目的是pytorch流程学习，因此数据仅选择了4张，分为2类，正常与新冠，训练集2张，验证集2张。标签信息存储于txt文件中。具体目录结构如下： ├─imgs │ ├─covid-19 │ │ auntminnie-a-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg │ │ ryct.2020200028.fig1a.jpeg │ │ │ └─no-finding │ 00001215_000.png │ 00001215_001.png │ └─labels train.txt valid.txt 建模思路 这是一个典型的图像分类任务，这里采用面向过程的思路给大家介绍如何进行代码编写。 step 1 数据 首先，需要编写代码完成数据的读取，变成模型能够读取的格式。这里涉及pytorch的dataset，dataloader，transforms等模块。以及需要清楚地知道pytorch的模型需要怎样的格式？数据模块需要完整的工作大体如下图所示： 首先，需要将数据在硬盘上的信息，如路径，标签读取并存储起来，然后被使用，这一步骤主要是通过COVID19Dataset这个类。类里有四个函数，除了Dataset类必须要实现的三个外，我们通过get_img_info函数实现读取硬盘中的路径、标签等信息，并存储到一个列表中。后续大家可以根据不同的任务情况在这个函数中修改，只要能获取到数据的信息，供\\_getitem__函数进行读取。 接着，使用dataloader进行封装，dataloader是一个数据加载器，提供诸多方法进行数据的获取，如设置一个batch获取几个样本，采用几个进程进行数据读取，是否对数据进行打乱等功能。 其次，还需要设置对图像进行预处理(Preprocess)的操作，这里为了演示，仅采用resize 和 totensor两个方法，并且图片只需要缩放到8*8的大小，并不需要224,256,448,512,1024等大尺寸。(totensor与下一小节内容强相关) step 2 模型 数据模块构建完毕，需要扔到模型里，因此我们需要构建神经网络模型，模型接收数据并前向传播处理，输出二分类概率向量。这时就需要用到nn.Module模块和nn下的各个网络层进行搭建模型，模型的搭建就像搭积木，一层一层的摞起来。模型完成的任务就如下图所示：下图示意图是一张分辨率为4*4的图像输入到模型中，模型经过运算，输出二分类概率。中间的“?\"是什么内容呢？ 这里，“？”是构建一个极其简单的卷积神经网络，仅仅包含两个网络层，第一个层是包含1个33卷积核的2d卷积，第二个层是两个神经元的全连接层（pytorch也叫linear层）。模型的输入被限制在了8\\8，原因在于linear层设置了输入神经元个数为36， 8*8与36之间是息息相关的，他们之间的关系是为何呢？这需要大家对卷积层有一定了解了。（大家可以改一下36，改为35，或者transforms_func中的resize改为9*9，看看会报什么错，这些错或许是大家今后经常会遇到的） step3 优化 模型可以完成前向传播之后，根据什么规则对模型的参数进行更新学习呢？这就需要损失函数和优化器的搭配了，损失函数用于衡量模型输出与标签之间的差异，并通过反向传播获得每个参数的梯度，有了梯度，就可以用优化器对权重进行更新。这里就要涉及各种LossFunction和optim中的优化器，以及学习率调整模块optim.lr_scheduler。 这里，采用的都是常用的方法：交叉熵损失函数（CrossEntropyLoss）、随机梯度下降法（SGD）和按固定步长下降学习率策略（StepLR）。 step4 迭代 有了模型参数更新的必备组件，接下来需要一遍又一遍的给模型喂数据，并且监控模型训练状态，这时候就需要for循环登场，不断的从dataloader里取出数据进行前向传播，反向传播，参数更新，观察loss、acc，周而复始。当达到满足的条件，如最大迭代次数、某指标达到某个值时，进行模型保存，并break循环，停止训练。 以上就是一个经典的面向过程式的代码编写，先考虑数据怎么读进来，读进来之后喂给的模型如何搭建，模型如何更新，模型如何迭代训练到满意。请大家结合代码一步一步的观察整体过程。 在经过几十个epoch的训练之后达到了100%，模型可以成功区分从未见过的两张图片：auntminnie-a-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg，00001215_000.png。 由于数据量少，随机性非常大，大家多运行几次，观察结果。不过本案例结果完全不重要！），可以看到模型的准确率（Accuracy）变化。 一系列问题 通过上述步骤及代码，虽然完成了一个图像分类任务，但其中很多细节想必大家还是弄不清楚，例如： 图像数据是哪用一行代码读取进来的？ transforms.Compose是如何工作对图像数据进行转换的？ ToTensor又有哪些操作？ 自己如何编写Dataset？ DataLoader有什么功能？如何使用？有什么需要注意的？ 模型如何按自己的数据流程搭建？ nn有哪些网络层可以调用？ 损失函数有哪些？ 优化器是如何更新model参数的？ 学习率调整有哪些方法？如何设置它们的参数？ model.train()与model.eval()作用是什么？ optimizer.zero_grad()是做什么？为什么要梯度清零？ scheduler.step() 作用是什么？应该放在哪个for循环里？ 等等 如果大家能有以上的问题提出，本小节的目的就达到了。大家有了模型训练的思路，对过程有了解，但是使用细节还需进一步学习，更多pytorch基础内容将会在后续章节一一解答。 下一小节我们将介绍流动在pytorch各个模块中的基础数据结构——Tensor（张量）。 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-2/2.3-datastruct-tensor.html":{"url":"chapter-2/2.3-datastruct-tensor.html","title":"2.3 核心数据结构——Tensor","keywords":"","body":"2.3 核心数据结构——Tensor 张量初认识 经过前两小节的铺垫，大家一定对pytorch有了初步认识，本小节就展开讲pytorch的核心数据结构——Tensor（张量）。Tensor中文翻译张量，是一个词不达意的名字。张量在不同学科中有不同的意义，在深度学习中张量表示的是一个多维数组，它是标量、向量、矩阵的拓展。标量是零维张量，向量是一维张量，矩阵是二维张量，一个RGB图像数组就是一个三维张量，第一维是图像高，第二维是图像的宽，第三维是图像颜色通道。 在pytorch中，有两个张量的相关概念极其容易混淆，分别是torch.Tensor和torch.tensor。其实，通过命名规范，可知道torch.Tensor是一个类, torch.tensor是一个函数。通常我们调用torch.tensor进行创建张量，而不直接调用torch.Tensor类进行创建。为了进一步区分两者，我们来看看它们代码实现。 torch.Tensor：类定义与torch/_tensor.py#L80，它继承torch._C._TensorBase，这里看到_C就知道要接触C++代码了。 跳转到torch/C/\\_init__.pyi #L839 可以看到： # Defined in torch/csrc/autograd/python_variable.cpp class _TensorBase(metaclass=_TensorMeta): requires_grad: _bool shape: Size 张量类的底层实现是在python_variable.cpp代码中，感兴趣的朋友可以进一步探究。 torch.tensor：pytorch的一个函数，用于将数据变为张量形式的数据，例如list, tuple, NumPy ndarray, scalar等。同样的，它的底层实现也是C++代码，我们可以跳转到函数定义，发现是torch_C_VariableFunctions.pyi文件（2.1节中介绍了.pyi文件是用于pyi文件是python用来校验数据类型的，其底层实现在对应的cpp代码中。 后续将不再区分Tensor和tensor，主要用小写tensor表示张量这个数据类型（数据结构）。 张量的作用 tensor之于pytorch等同于ndarray之于numpy，它是pytorch中最核心的数据结构，用于表达各类数据，如输入数据、模型的参数、模型的特征图、模型的输出等。这里边有一个很重要的数据，就是模型的参数。对于模型的参数，我们需要它进行更新，而更新是需要记录它的梯度，梯度的记录功能正是被张量所实现的（求梯度是autograd实现的）。 张量的历史演变 讲tensor结构之前，还需要介绍一小段历史，那就是Variable与Tensor。在0.4.0版本之前，Tensor需要经过Variable的包装才能实现自动求导。从0.4.0版本开始，torch.Tensor与torch.autograd.Variable合并，torch.Tensor拥有了跟踪历史操作的功能。虽然Variable仍可用，但Variable返回值已经是一个Tensor（原来返回值是Variable），所以今后无需再用Variable包装Tensor。 虽然Variable的概念已经被摒弃，但是了解其数据结构对理解Tensor还是有帮助的。Variable不仅能对Tensor的包装，而且能记录生成Tensor的运算（这是自动求导的关键）。在Variable对象中主要包含5个属性：data，grad，grad_fn，is_leaf，requires_grad data: 保存的是具体数据，即被包装的Tensor； grad: data对应的梯度，形状与data一致； grad_fn: 记录创建该Tensor时用到的Function，该Function在反向传播计算中使用，因此是自动求导的关键； requires_grad: 用来指示是否需要梯度； is_leaf: 用来指示是否是叶子结点，为叶子结点时，反向传播结束，其梯度仍会保存，非叶子结点的梯度被释放，以节省内存。 从Variable的主要属性中可以发现，除了data外，grad，grad_fn，is_leaf和requires_grad都是为计算梯度服务，所以Variable在torch.autogard包中自然不难理解。 但是我们的数据载体是tensor，每次需要自动求导，都要用Variable包装，这明显太过繁琐，于是PyTorch从0.4.0版将torch.Tensor与torch.autograd.Variable合并。 张量的结构 tensor是一个类，我们先来认识它有哪些属性，再去观察它有哪些方法函数可使用。 Tensor主要有以下八个主要属性，data，dtype，shape，device，grad，grad_fn，is_leaf，requires_grad。 data：多维数组，最核心的属性，其它属性都是为其服务的; dtype：多维数组的数据类型，tensor数据类型如下，常用到的三种已经用红框标注出来； shape：多维数组的形状; device: tensor所在的设备，cpu或cuda; grad，grad_fn，is_leaf和requires_grad就与Variable一样，都是梯度计算中所用到的。 张量的属性还有很多，大家可以通过Pycharm的debug功能进行查看 更多关于张量的概念背景，请查看官方文档，下一小节，我们进行张量的操作介绍。 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "},"chapter-2/2.4-method-tensor.html":{"url":"chapter-2/2.4-method-tensor.html","title":"2.4 张量的相关函数","keywords":"","body":"2.4 张量的相关函数 接下来开始学习各类张量的api，主要参考官方文档，通过右边目录栏可以看出有以下几个部分。 torchTensors Generators Random sampling Serialization Parallelism Locally disabling gradient computation Math operations Utilities 里面有上百个函数，这里只挑高频使用的进行讲解，建议大家自行浏览一遍官方文档，看看都有哪些功能，便于今后使用到的时候不必重复造轮子。 张量的创建 直接创建 torch.tensor torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) data(array_like) - tensor的初始数据，可以是list, tuple, numpy array, scalar或其他类型。 dtype(torch.dtype, optional) - tensor的数据类型，如torch.uint8, torch.float, torch.long等 device (torch.device, optional) – 决定tensor位于cpu还是gpu。如果为None，将会采用默认值，默认值在torch.set_default_tensor_type()中设置，默认为 cpu。 requires_grad (bool, optional) – 决定是否需要计算梯度。 pin_memory (bool, optional) – 是否将tensor存于锁页内存。这与内存的存在方式有关，通常为False。 import torch import numpy as np l = [[1., -1.], [1., -1.]] t_from_list = torch.tensor(l) arr = np.array([[1, 2, 3], [4, 5, 6]]) t_from_array = torch.tensor(arr) print(t_from_list, t_from_list.dtype) print(t_from_array, t_from_array.dtype) tensor([[ 1., -1.], ​ [ 1., -1.]]) torch.float32 tensor([[1, 2, 3], ​ [4, 5, 6]]) torch.int64 可以看到t_from_list是float32类型，而t_from_array是int64类型。如果想让tensor是其他数据类型，可以在创建tensor时使用dytpe参数确定数据类型。 import torch import numpy as np arr = np.array([[1, 2, 3], [4, 5, 6]]) t_from_array = torch.tensor(arr, dtype=torch.uint8) print(t_from_array) tensor([[1, 2, 3], ​ [4, 5, 6]], dtype=torch.uint8) torch.from_numpy 还有一种常用的通过numpy创建tensor方法是torch.from_numpy()。这里需要特别注意的是，创建的tensor和原array共享同一块内存（The returned tensor and ndarray share the same memory. ），即当改变array里的数值，tensor中的数值也会被改变。 import torch import numpy as np arr = np.array([[1, 2, 3], [4, 5, 6]]) t_from_numpy = torch.from_numpy(arr) print(\"numpy array: \", arr) print(\"tensor : \", t_from_numpy) print(\"\\n修改arr\") arr[0, 0] = 0 print(\"numpy array: \", arr) print(\"tensor : \", t_from_numpy) print(\"\\n修改tensor\") t_from_numpy[0, 0] = -1 print(\"numpy array: \", arr) print(\"tensor : \", t_from_numpy) > > numpy array: [[1 2 3] [4 5 6]] tensor : tensor([[1, 2, 3], ​ [4, 5, 6]]) 修改arr numpy array: [[0 2 3] [4 5 6]] tensor : tensor([[0, 2, 3], ​ [4, 5, 6]]) 修改tensor numpy array: [[-1 2 3] [ 4 5 6]] tensor : tensor([[-1, 2, 3], ​ [ 4, 5, 6]]) 可以看到虽然只改变了arr的值，但是tensor中的data也被改变了，这一点在使用过程中需要注意。 依数值创建 torch.zeros torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：依给定的size创建一个全0的tensor，默认数据类型为torch.float32（也称为torch.float）。 主要参数： layout(torch.layout, optional) - 参数表明张量在内存中采用何种布局方式。常用的有torch.strided, torch.sparse_coo等。 out(tensor, optional) - 输出的tensor，即该函数返回的tensor可以通过out进行赋值，请看例子。 example: import torch o_t = torch.tensor([1]) t = torch.zeros((3, 3), out=o_t) print(t, '\\n', o_t) print(id(t), id(o_t)) > > tensor([[0, 0, 0], ​ [0, 0, 0], ​ [0, 0, 0]]) tensor([[0, 0, 0], ​ [0, 0, 0], ​ [0, 0, 0]]) 4925603056 4925603056 可以看到，通过torch.zeros创建的张量不仅赋给了t，同时赋给了o_t，并且这两个张量是共享同一块内存，只是变量名不同。 torch.zeros_like torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) 功能：依input的size创建全0的tensor。 主要参数： input(Tensor) - 创建的tensor与intput具有相同的形状。 example: import torch t1 = torch.tensor([[1., -1.], [1., -1.]]) t2 = torch.zeros_like(t1) print(t2) tensor([[0., 0.], ​ [0., 0.]]) 除了创建全0还有创建全1的tensor，使用方法是一样的，这里就不赘述。 torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：依给定的size创建一个全1的tensor。 torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) 功能：依input的size创建全1的tensor。 torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：依给定的size创建一个值全为fill_value的tensor。 主要参数: siz (int...) - tensor的形状。 fill_value - 所创建tensor的值 out(tensor, optional) - 输出的tensor，即该函数返回的tensor可以通过out进行赋值。 example: import torch print(torch.full((2, 3), 3.141592)) torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) torch.full_like之于torch.full等同于torch.zeros_like之于torch.zeros，因此不再赘述。 torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：创建等差的1维张量，长度为 (end-start)/step，需要注意数值区间为[start, end)。 主要参数： start (Number) – 数列起始值，默认值为0。the starting value for the set of points. Default: 0. end (Number) – 数列的结束值。 step (Number) – 数列的等差值，默认值为1。 out (Tensor, optional) – 输出的tensor，即该函数返回的tensor可以通过out进行赋值。 example: imort torch print(torch.arange(1, 2.51, 0.5)) torch.range()函数就不推荐及介绍了，因为官网说了“This function is deprecated in favor of torch.arange().” torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：创建均分的1维张量，长度为steps，区间为[start, end]。 主要参数： start (float) – 数列起始值。 end (float) – 数列结束值。 steps (int) – 数列长度。 example: print(torch.linspace(3, 10, steps=5)) print(torch.linspace(1, 5, steps=3)) torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：创建对数均分的1维张量，长度为steps, 底为base。 主要参数： start (float) – 确定数列起始值为base^start end (float) – 确定数列结束值为base^end steps (int) – 数列长度。 base (float) - 对数函数的底，默认值为10，此参数是在pytorch 1.0.1版本之后加入的。 example: torch.logspace(start=0.1, end=1.0, steps=5) torch.logspace(start=2, end=2, steps=1, base=2) torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)** 功能：创建单位对角矩阵。 主要参数： n (int) - 矩阵的行数 m (int, optional) - 矩阵的列数，默认值为n，即默认创建一个方阵 example: import torch print(torch.eye(3)) print(torch.eye(3, 4)) torch.empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) 功能：依size创建“空”张量，这里的“空”指的是不会进行初始化赋值操作。 主要参数： size (int...) - 张量维度 pin_memory (bool, optional) - pinned memory 又称page locked memory，即锁页内存，该参数用来指示是否将tensor存于锁页内存，通常为False，若内存足够大，建议设置为Ture，这样在转到GPU时会快一些。 torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) 功能：torch.empty_like之于torch.empty等同于torch.zeros_like之于torch.zeros，因此不再赘述。 torch.empty_strided(size, stride, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) 功能：依size创建“空”张量，这里的“空”指的是不会进行初始化赋值操作。 主要参数： stride (tuple of python:ints) - 张量存储在内存中的步长，是设置在内存中的存储方式。 size (int...) - 张量维度 pin_memory (bool, optional) - 是否存于锁页内存。 依概率分布创建 torch.normal(mean, std, out=None) 功能：为每一个元素以给定的mean和std用高斯分布生成随机数 主要参数： mean (Tensor or Float) - 高斯分布的均值， std (Tensor or Float) - 高斯分布的标准差 特别注意事项： mean和std的取值分别有2种，共4种组合，不同组合产生的效果也不同，需要注意 mean为张量，std为张量，torch.normal(mean, std, out=None)，每个元素从不同的高斯分布采样，分布的均值和标准差由mean和std对应位置元素的值确定； mean为张量，std为标量，torch.normal(mean, std=1.0, out=None)，每个元素采用相同的标准差，不同的均值； mean为标量，std为张量，torch.normal(mean=0.0, std, out=None)， 每个元素采用相同均值，不同标准差； mean为标量，std为标量，torch.normal(mean, std, size, *, out=None) ，从一个高斯分布中生成大小为size的张量； example1 import mean = torch.arange(1, 11.) std = torch.arange(1, 0, -0.1) normal = torch.normal(mean=mean, std=std) print(\"mean: {}, \\nstd: {}, \\nnormal: {}\".format(mean, std, normal)) mean: tensor([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]), std: tensor([1.0000, 0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000, ​ 0.1000]), normal: tensor([ 1.3530, -1.3498, 3.0021, 5.1200, 3.9818, 5.0163, 6.9272, 8.1171, ​ 9.0623, 10.0621]) 1.3530是通过均值为1，标准差为1的高斯分布采样得来， -1.3498是通过均值为2，标准差为0.9的高斯分布采样得来，以此类推 torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：在区间[0, 1)上，生成均匀分布。 主要参数： size (int...) - 创建的张量的形状 torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) torch.rand_like之于torch.rand等同于torch.zeros_like之于torch.zeros，因此不再赘述。 torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：在区间[low, high)上，生成整数的均匀分布。 主要参数： low (int, optional) - 下限。 high (int) – 上限，主要是开区间。 size (tuple) – 张量的形状。 example print(torch.randint(3, 10, (2, 2))) torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：torch.randint_like之于torch.randint等同于torch.zeros_like之于torch.zeros，因此不再赘述。 torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 功能：生成形状为size的标准正态分布张量。 主要参数： size (int...) - 张量的形状 torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) 功能：torch.rafndn_like之于torch_randn等同于torch.zeros_like之于torch.zeros，因此不再赘述。 torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) 功能：生成从0到n-1的随机排列。perm == permutation torch.bernoulli(input, *, generator=None, out=None) 功能：以input的值为概率，生成伯努力分布（0-1分布，两点分布）。 主要参数： input (Tensor) - 分布的概率值，该张量中的每个值的值域为[0-1] example: import torch p = torch.empty(3, 3).uniform_(0, 1) b = torch.bernoulli(p) print(\"probability: \\n{}, \\nbernoulli_tensor:\\n{}\".format(p, b)) probability: tensor([[0.7566, 0.2899, 0.4688], ​ [0.1662, 0.8341, 0.9572], ​ [0.6060, 0.4685, 0.6366]]), bernoulli_tensor: tensor([[0., 0., 1.], ​ [1., 1., 1.], ​ [1., 1., 1.]]) 张量的操作 熟悉numpy的朋友应该知道，Tensor与numpy的数据结构很类似，不仅数据结构类似，操作也是类似的，接下来介绍Tensor的常用操作。由于操作函数很多，这里就不一一举例，仅通过表格说明各个函数作用，详细介绍可查看官方文档 cat 将多个张量拼接在一起，例如多个特征图的融合可用。 concat 同cat, 是cat()的别名。 conj 返回共轭复数。 chunk 将tensor在某个维度上分成n份。 dsplit 类似numpy.dsplit().， 将张量按索引或指定的份数进行切分。 column_stack 水平堆叠张量。即第二个维度上增加，等同于torch.hstack。 dstack 沿第三个轴进行逐像素（depthwise）拼接。 gather 高级索引方法，目标检测中常用于索引bbox。在指定的轴上，根据给定的index进行索引。强烈推荐看example。 hsplit 类似numpy.hsplit()，将张量按列进行切分。若传入整数，则按等分划分。若传入list，则按list中元素进行索引。例如：[2, 3] and dim=0 would result in the tensors input[:2], input[2:3], and input[3:]. hstack 水平堆叠张量。即第二个维度上增加，等同于torch.column_stack。 index_select 在指定的维度上，按索引进行选择数据，然后拼接成新张量。可知道，新张量的指定维度上长度是index的长度。 masked_select 根据mask（0/1, False/True 形式的mask）索引数据，返回1-D张量。 movedim 移动轴。如0，1轴交换：torch.movedim(t, 1, 0) . moveaxis 同movedim。Alias for torch.movedim().（这里发现pytorch很多地方会将dim和axis混用，概念都是一样的。） narrow 变窄的张量？从功能看还是索引。在指定轴上，设置起始和长度进行索引。例如：torch.narrow(x, 0, 0, 2)， 从第0个轴上的第0元素开始，索引2个元素。x[0:0+2, ...] nonzero 返回非零元素的index。torch.nonzero(torch.tensor([1, 1, 1, 0, 1])) 返回tensor([[ 0], [ 1], [ 2], [ 4]])。建议看example，一看就明白，尤其是对角线矩阵的那个例子，太清晰了。 permute 交换轴。 reshape 变换形状。 row_stack 按行堆叠张量。即第一个维度上增加，等同于torch.vstack。Alias of torch.vstack(). scatter scatter_(dim, index, src, reduce=None) → Tensor。将src中数据根据index中的索引按照dim的方向填进input中。这是一个十分难理解的函数，其中index是告诉你哪些位置需要变，src是告诉你要变的值是什么。这个就必须配合例子讲解，请跳转到本节底部进行学习。 scatter_add 同scatter一样，对input进行元素修改，这里是 +=， 而scatter是直接替换。 split 按给定的大小切分出多个张量。例如：torch.split(a, [1,4])； torch.split(a, 2) squeeze 移除张量为1的轴。如t.shape=[1, 3, 224, 224]. t.squeeze().shape -> [3, 224, 224] stack 在新的轴上拼接张量。与hstack\\vstack不同，它是新增一个轴。默认从第0个轴插入新轴。 swapaxes Alias for torch.transpose().交换轴。 swapdims Alias for torch.transpose().交换轴。 t 转置。 take 取张量中的某些元素，返回的是1D张量。torch.take(src, torch.tensor([0, 2, 5]))表示取第0,2,5个元素。 take_along_dim 取张量中的某些元素，返回的张量与index维度保持一致。可搭配torch.argmax(t)和torch.argsort使用，用于对最大概率所在位置取值，或进行排序，详见官方文档的example。 tensor_split 切分张量，核心看indices_or_sections变量如何设置。 tile 将张量重复X遍，X遍表示可按多个维度进行重复。例如：torch.tile(y, (2, 2)) transpose 交换轴。 unbind 移除张量的某个轴，并返回一串张量。如[[1], [2], [3]] --> [1], [2], [3] 。把行这个轴拆了。 unsqueeze 增加一个轴，常用于匹配数据维度。 vsplit 垂直切分。 vstack 垂直堆叠。 where 根据一个是非条件，选择x的元素还是y的元素，拼接成新张量。看案例可瞬间明白。 scater_ scater是将input张量中的部分值进行替换。公式如下： self[index[i][j][k]][j][k] = src[i][j][k] # if dim == 0 self[i][index[i][j][k]][k] = src[i][j][k] # if dim == 1 self[i][j][index[i][j][k]] = src[i][j][k] # if dim == 2 设计两个核心问题： input哪个位置需要替换？ 替换成什么？ 答： 从公式可知道，依次从index中找到元素放到dim的位置，就是input需要变的地方。 变成什么呢？ 从src中找，src中与index一样位置的那个元素值放到input中。 案例1： >>> src = torch.arange(1, 11).reshape((2, 5)) >>> src tensor([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10]]) >>> index = torch.tensor([[0, 1, 2, 0]]) >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src) tensor([[1, 0, 0, 4, 0], [0, 2, 0, 0, 0], [0, 0, 3, 0, 0]]) dim=0, 所以行号跟着index的元素走。其它跟index的索引走。 第一步：找到index的第一个元素index[0, 0]是0， 那么把src[0, 0]（是1）放到input[0, 0]第二步：找到index的第二个元素index[0, 1]是1， 那么把src[0, 1]（是2）放到input[1, 1]第三步：找到index的第三个元素index[0, 2]是2， 那么把src[0, 2]（是3）放到input[2, 2]第四步：找到index的第四个元素index[0, 3]是0， 那么把src[0, 3]（是4）放到input[0, 3] 案例2： >>> src = torch.arange(1, 11).reshape((2, 5)) >>> src tensor([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10]]) >>> index = torch.tensor([[0, 2, 4], [1, 2, 3]]) >>> index tensor([[0, 2, 4], [1, 2, 3]]) >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src) tensor([[1, 0, 2, 0, 3], [0, 6, 7, 8, 0], [0, 0, 0, 0, 0]]) dim=1：告诉input（零矩阵）的索引，沿着列进行索引，行根据index走。 index：2*3，告诉input（零矩阵），你的哪些行是要被替换的。 src：input要替换成什么呢？从src里找，怎么找？通过index的索引对应的找。 第一步：找到index的第一个元素index[0, 0]是0， 那么把src[0, 0]（是1）放到input[0, 0]第二步：找到index的第二个元素index[0, 1]是2， 那么把src[0, 1]（是2）放到input[0, 2]第三步：找到index的第三个元素index[0, 2]是4， 那么把src[0, 2]（是3）放到input[0, 4]第四步：找到index的第四个元素index[1, 0]是1， 那么把src[1, 0]（是6）放到input[1, 1]第五步：找到index的第五个元素index[1, 1]是2， 那么把src[1, 1]（是7）放到input[1, 2]第六步：找到index的第六个元素index[1, 2]是3， 那么把src[1, 2]（是8）放到input[1, 3] 这里可以看到 index的元素是决定input的哪个位置要变 变的值是从src上对应于index的索引上找。可以看到src的索引与index的索引保持一致的 案例3：one-hot的生成 >>> label = torch.arange(3).view(-1, 1) >>> label tensor([[0], [1], [2]]) >>> torch.zeros(3, 3).scatter_(1, label, 1) tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 第一步：找到index的第一个元素index[0, 0]是0， 那么把src[0, 0]（是1）放到input[0, 0] 第二步：找到index的第二个元素index[1, 0]是1， 那么把src[1, 0]（是1）放到input[1, 1] 第三步：找到index的第三个元素index[2, 0]是2， 那么把src[2, 0]（是1）放到input[2, 2] （one-hot的案例不利于理解scater函数，因为它的行和列是一样的。。。其实input[x, y] 中的x,y是有区别的，x是根据index走，y是根据index的元素值走的，而具体的值是根据src的值。） 张量的随机种子 随机种子（random seed）是编程语言中基础的概念，大多数编程语言都有随机种子的概念，它主要用于实验的复现。针对随机种子pytorch也有一些设置函数。 seed 获取一个随机的随机种子。Returns a 64 bit number used to seed the RNG. manual_seed 手动设置随机种子，建议设置为42，这是近期一个玄学研究。说42有效的提高模型精度。当然大家可以设置为你喜欢的，只要保持一致即可。 initial_seed 返回初始种子。 get_rng_state 获取随机数生成器状态。Returns the random number generator state as a torch.ByteTensor. set_rng_state 设定随机数生成器状态。这两怎么用暂时未知。Sets the random number generator state. 以上均是设置cpu上的张量随机种子，在cuda上是另外一套随机种子，如torch.cuda.manual_seed_all(seed)， 这些到cuda模块再进行介绍，这里只需要知道cpu和cuda上需要分别设置随机种子。 张量的数学操作 张量还提供大量数学操作，估计了一下，有快一百个函数，这里就不再一一分析，只需要知道有哪几大类，用到的时候来查吧。 Pointwise Ops： 逐元素的操作，如abs, cos, sin, floor, floor_divide, pow等 Reduction Ops: 减少元素的操作，如argmax, argmin, all, any, mean, norm, var等 Comparison Ops：对比操作， 如ge, gt, le, lt, eq, argsort, isnan, topk, Spectral Ops: 谱操作，如短时傅里叶变换等各类信号处理的函数。 Other Operations：其它， clone， diag，flip等 BLAS and LAPACK Operations：BLAS（Basic Linear Algebra Subprograms）基础线性代数）操作。如, addmm, dot, inner, svd等。 小结 本节介绍了张量主要的操作函数，并归类到各个小结，这些仅是张量的部分操作，更多操作还请大家多多看官方文档。对于张量，主要是要理解2.3小节中张量的结构以及作用，对于它的操作就像numpy一样简单易用。 下一节就开始讲解pytorch的核心——autograd，autograd也是现代深度学习框架的核心，是实现自动微分的具体实现。 Copyright © TingsongYu 2021 all right reserved，powered by Gitbook文件修订时间： 2022年01月02日11:48:10 "}}